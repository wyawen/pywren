{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywren\n",
    "import numpy as np\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "import pywren.storage as storage\n",
    "import boto3\n",
    "import pickle \n",
    "import sys\n",
    "from redis import Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration \n",
    "num_workers = 10 #20 #100\n",
    "bucket_name = 'terasort-yawen-lambda'\n",
    "\n",
    "# the file to be sorted should be partitioned into \"num_worker\" number of files \n",
    "# as inputs to the map stage; \n",
    "# specify directory that contains files to be sorted: input1, input2, etc. \n",
    "file_path_local = 'inputs/input_10_30M/' \n",
    "file_name = 'input'\n",
    "concat_file_name = 'inputs/input_10_30M/input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "#boto3 already configured \n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# upload n input files to S3 (inputs to the mapper stage)\n",
    "for i in range(num_workers):\n",
    "    result = s3_client.put_object(\n",
    "        Bucket = bucket_name,\n",
    "        Body = open(file_path_local + file_name + str(i), 'rb'),\n",
    "        Key = file_name+str(i)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## sample[i âˆ’ 1] <= key < sample[i] is sent to reduce i\n",
    "def get_sample_keys(file_path, num_workers):\n",
    "    ## Open the file with read only permit\n",
    "    f = open(file_path, \"r\")\n",
    "\n",
    "    ## use readlines to read all lines in the file\n",
    "    ## The variable \"lines\" is a list containing all lines\n",
    "    lines = f.readlines()\n",
    "\n",
    "    key_list = []\n",
    "\n",
    "    for line in lines: \n",
    "        data = line.split(\"  \")\n",
    "        key = data[0]\n",
    "        key_list.append(key)\n",
    "\n",
    "    key_list.sort()\n",
    "    length = len(key_list)\n",
    "    #print \"num records: \" + str(length)\n",
    "    n = num_workers\n",
    "    key_range = length/n\n",
    "    index = 0\n",
    "    sample_key_list = []\n",
    "    for i in range(1, n+1): #1,2,3\n",
    "        if (i==n):\n",
    "            index = length -1\n",
    "            sample_key_list.append(key_list[length-1])\n",
    "        else:\n",
    "            index += key_range\n",
    "            sample_key_list.append(key_list[index])\n",
    "        #print index\n",
    "    \n",
    "    return sample_key_list\n",
    "\n",
    "#sample_keys = get_sample_keys('input_files/input0', num_workers)\n",
    "#sample_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# partition stage: partition input data into n groups \n",
    "def mapper(data):\n",
    "    import os\n",
    "    from redis import Redis\n",
    "\n",
    "    \n",
    "    id = data[0]\n",
    "    n = num_workers = data[1]\n",
    "    bucket_name = data[2]\n",
    "    sample_keys = data[3]\n",
    "\n",
    "    t0=time.time()\n",
    "    #[s3] read from input file: input<id> \n",
    "    s3 = boto3.resource('s3')\n",
    "    key = 'input'+str(id)\n",
    "    file_local = '/tmp/input_tmp'\n",
    "    s3.Bucket(bucket_name).download_file(key, file_local)\n",
    "    t1=time.time()    \n",
    "        \n",
    "    #partition \n",
    "    with open(file_local, \"r\") as f: \n",
    "        lines = f.readlines() #each line contains a 100B record\n",
    "    os.remove(file_local)\n",
    "    p_list = [[] for x in xrange(n)]  #list of n partitions\n",
    "    for line in lines:\n",
    "        data = line.split(\"  \")\n",
    "        '''\n",
    "        if len(data) != 3:\n",
    "            data[0] = data[0]+data[1]\n",
    "            data[1] = data[2]\n",
    "            data[2] = data[3]\n",
    "        '''\n",
    "        index = 0\n",
    "        while data[0] > sample_keys[index]:\n",
    "            index += 1\n",
    "        p_list[index].append(line)\n",
    "    t2=time.time()\n",
    "    \n",
    "    \n",
    "    #write to output files: shuffle<id 0> shuffle<id 1> shuffle<id num_workers-1>\n",
    "    f_list = [] #output file list\n",
    "    \n",
    "    gateway_ip = '54.212.247.168'\n",
    "    gateway_port = '8888'\n",
    "    bind_port = 40000+id \n",
    "    redis = Redis(gateway_ip, int(gateway_port), bind_port=int(bind_port))\n",
    "    for i in range(n):\n",
    "        key = 'shuffle' + str(id) + str(i)\n",
    "        result = redis.set(key, pickle.dumps(p_list[i]))\n",
    "    t3 = time.time()\n",
    "    \n",
    "    #return time spent (in sec) writing intermediate files \n",
    "    return [t1-t0, t2-t1, t3-t2] #read input, compute, write shuffle \n",
    "\n",
    "#mapper([2, num_workers, bucket_name, sample_keys])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# sort stage: merge n sets of data & sort \n",
    "def reducer(data):\n",
    "    import os\n",
    "    from redis import Redis\n",
    "    \n",
    "    id = data[0]\n",
    "    n = num_workers = data[1]\n",
    "    bucket_name = data[2]\n",
    "    \n",
    "    #read from input file: shuffle<0 id> shuffle<1 id> ... shuffle<id num_workers-1>\n",
    "    t0 = time.time()\n",
    "    gateway_ip = '54.212.247.168'\n",
    "    gateway_port = '8888'\n",
    "    bind_port = 50000+id\n",
    "    redis = Redis(gateway_ip, int(gateway_port), bind_port=int(bind_port))\n",
    "    lines_list = []\n",
    "    for i in range(n):\n",
    "        key = 'shuffle'+ str(0) + str(id)\n",
    "        body = redis.get(key)\n",
    "        if body == None:\n",
    "            return -1\n",
    "        lines = pickle.loads(body)\n",
    "        lines_list.append(lines)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    #merge & sort \n",
    "    merged_lines = sum(lines_list, [])\n",
    "    tuples_list = []\n",
    "    for line in merged_lines:\n",
    "        data = line.split('  ')\n",
    "        tuples_list.append((data[0], data[1]+'  '+data[2]))\n",
    "    \n",
    "    sorted_tuples_list = sorted(tuples_list, key=lambda x: x[0])\n",
    "    t2=time.time()\n",
    "    \n",
    "    #[s3] write to output file: output<id>  \n",
    "    t_output0=time.time()\n",
    "    s3_client = boto3.client('s3')\n",
    "    file_name = 'sorted_output' + str(id)\n",
    "    result = s3_client.put_object(\n",
    "        Bucket = bucket_name,\n",
    "        Body = pickle.dumps(sorted_tuples_list),\n",
    "        Key = file_name\n",
    "    )\n",
    "    t3=time.time()\n",
    "    \n",
    "    #return time (in sec) spent reading intermediate files\n",
    "    return [t1-t0, t2-t1, t3-t2] #read shuffle, compute, write output \n",
    "\n",
    "#reducer([0, num_workers, bucket_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_reducer(data):\n",
    "    import os\n",
    "    \n",
    "    t0=time.time()\n",
    "    n = data[0]\n",
    "    bucket_name = data[1]\n",
    "    \n",
    "    #read from input file: shuffle<0 id> shuffle<1 id> ... shuffle<id num_workers-1>\n",
    "    s3_client = boto3.client('s3')\n",
    "    tuples_list = []\n",
    "    for i in range(n):\n",
    "        key = 'sorted_output'+ str(i)\n",
    "        body = s3_client.get_object(Bucket=bucket_name, Key=key)['Body'].read()\n",
    "        tuples_list += pickle.loads(body)\n",
    "        \n",
    "    with open('./sorted_output', 'w') as f:\n",
    "        for data in tuples_list: \n",
    "            f.write(str(data[0])+'  '+str(data[1]))\n",
    "    t1=time.time()\n",
    "    return (t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "wrenexec = pywren.default_executor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "map_data_list = []\n",
    "reduce_data_list = []\n",
    "\n",
    "sample_keys = get_sample_keys(concat_file_name, num_workers)\n",
    "\n",
    "for i in range(num_workers):\n",
    "    map_data_list.append([i, num_workers, bucket_name, sample_keys])\n",
    "    reduce_data_list.append([i, num_workers, bucket_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "futures = wrenexec.map(mapper, map_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_map = pywren.get_all_results(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "futures = wrenexec.map(reducer, reduce_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_reduce = pywren.get_all_results(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map:\n",
      "read input: 0.21368598938\n",
      "compute: 0.0870207150777\n",
      "write inter: 3.1236846447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.21266603469848633, 0.09230589866638184, 3.2602360248565674],\n",
       " [0.23511099815368652, 0.07327914237976074, 2.75010085105896],\n",
       " [0.19328093528747559, 0.09547710418701172, 3.3607170581817627]]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_io = []\n",
    "t_comp = []\n",
    "t_inter = []\n",
    "for r in results_map:\n",
    "    t_io.append(r[0])\n",
    "    t_comp.append(r[1])\n",
    "    t_inter.append(r[2])\n",
    "print \"map:\"\n",
    "print \"read input: \" + str(sum(t_io) / len(t_io))\n",
    "print \"compute: \" + str(sum(t_comp) / len(t_comp))\n",
    "print \"write inter: \" + str(sum(t_inter) / len(t_inter))\n",
    "\n",
    "# returns time spent (in sec) writing intermediate data in each mapper \n",
    "results_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce:\n",
      "read inter: 3.55727163951\n",
      "compute: 0.679601669312\n",
      "write output: 7.11767133077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.683604001998901, 1.3008201122283936, 7.160967826843262],\n",
       " [1.5085079669952393, 0.29518890380859375, 7.957313060760498],\n",
       " [4.479702949523926, 0.442795991897583, 6.2347331047058105]]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns time spent (in sec) reading intermediate data in each reducer \n",
    "\n",
    "t_io = []\n",
    "t_comp = []\n",
    "t_inter = []\n",
    "for r in results_reduce:\n",
    "    t_io.append(r[2])\n",
    "    t_comp.append(r[1])\n",
    "    t_inter.append(r[0])\n",
    "print \"reduce:\"\n",
    "print \"read inter: \" + str(sum(t_inter) / len(t_inter))\n",
    "print \"compute: \" + str(sum(t_comp) / len(t_comp))\n",
    "print \"write output: \" + str(sum(t_io) / len(t_io))\n",
    "results_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final stage: concatenate outputs from the reduce/sort stage to form a single sorted output file\n",
    "#final_reducer([num_workers,bucket_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
