{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywren\n",
    "import numpy as np\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "import pywren.storage as storage\n",
    "import boto3\n",
    "import pickle \n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import redis\n",
    "from rediscluster import StrictRedisCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition stage: partition input data into n groups \n",
    "def mapper(data):\n",
    "    id = data[0] #+ 2000\n",
    "    n = num_workers = data[1]\n",
    "    bucket_name = data[2]\n",
    "    sample_keys = data[3]\n",
    "    path = data[4]\n",
    "\n",
    "    t0=time.time()\n",
    "    #[s3] read from input file: input<id> \n",
    "    s3 = boto3.resource('s3')\n",
    "    file_local = '/tmp/input_tmp'\n",
    "    lines = []\n",
    "    for i in range(4):\n",
    "        i += id*4\n",
    "        key = path + 'input' + str(i)\n",
    "        s3.Bucket(bucket_name).download_file(key, file_local)\n",
    "        with open(file_local, \"r\") as f: \n",
    "            lines += f.readlines() #each line contains a 100B record\n",
    "        os.remove(file_local)    \n",
    "    t1=time.time() \n",
    "        \n",
    "    #partition \n",
    "    p_list = [[] for x in xrange(2500)]  #list of n partitions  #hardcode\n",
    "    for line in lines: \n",
    "        key1 = ord(line[0])-32 # key range 32-126\n",
    "        key2 = ord(line[1])-32\n",
    "        #126-32+1=95\n",
    "        #2500/95 ~ 26.3\n",
    "        index = int(26.3*(key1+key2/95.0))  #128*19+1/(128/19) \n",
    "        p_list[index].append(line)\n",
    "    \n",
    "    t1_2=time.time()\n",
    "\n",
    "    #test1\n",
    "    file_tmp = '/tmp/tmp'\n",
    "    for i in range(2500):\n",
    "        with open(file_tmp, \"w+\") as f:\n",
    "            f.writelines(p_list[i])\n",
    "            f.seek(0)\n",
    "            p_list[i] = f.read()\n",
    "        os.remove(file_tmp)\n",
    "        \n",
    "    t2=time.time()\n",
    "    \n",
    "    #write to output files: shuffle<id 0> shuffle<id 1> shuffle<id num_workers-1>    \n",
    "    startup_nodes = [{\"host\": \"rediscluster1.a9ith3.clustercfg.usw2.cache.amazonaws.com\", \"port\": \"6379\"}]\n",
    "    redis_client = StrictRedisCluster(startup_nodes=startup_nodes, decode_responses=True, skip_full_coverage_check=True)\n",
    "    \n",
    "    for i in range(2500): #hardcode\n",
    "        key = 'shuffle' + str(id) + str(i)\n",
    "        result = redis_client.set(key, p_list[i])\n",
    "    t3=time.time()\n",
    "\n",
    "    #return time spent (in sec) writing intermediate files \n",
    "    return [t1-t0, t1_2-t1, t3-t2, t2-t1_2] #read input, compute, write shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort stage: merge n sets of data & sort \n",
    "def reducer(data):\n",
    "    id = data[0]\n",
    "    n = num_workers = data[1]\n",
    "    bucket_name = data[2]\n",
    "    \n",
    "    #read from input file: shuffle<0 id> shuffle<1 id> ... shuffle<id num_workers-1>\n",
    "    t0 = time.time()\n",
    "    startup_nodes = [{\"host\": \"rediscluster1.a9ith3.clustercfg.usw2.cache.amazonaws.com\", \"port\": \"6379\"}]\n",
    "    redis_client = StrictRedisCluster(startup_nodes=startup_nodes, decode_responses=True, skip_full_coverage_check=True)\n",
    "    \n",
    "    all_lines = []\n",
    "    for i in range(2500): #hardcode\n",
    "        key = 'shuffle'+ str(i) + str(id)\n",
    "        body = redis_client.get(key)\n",
    "        if body == None:\n",
    "            return -1\n",
    "        #lines = pickle.loads(body)\n",
    "        #all_lines += lines\n",
    "        all_lines.append(body)\n",
    "    t1 = time.time()\n",
    "        \n",
    "    #test1\n",
    "    file_tmp = '/tmp/tmp'\n",
    "    for i in range(2500):\n",
    "        with open(file_tmp, \"w+\") as f:\n",
    "            f.write(all_lines[i])\n",
    "            f.seek(0)\n",
    "            all_lines[i] = f.readlines()\n",
    "        os.remove(file_tmp)\n",
    "        \n",
    "    t1_2 = time.time()\n",
    "    \n",
    "    #merge & sort \n",
    "    for i in range(len(all_lines)):\n",
    "        all_lines[i] = (all_lines[i][:10], all_lines[i][12:])\n",
    "    all_lines.sort(key=lambda x: x[0])\n",
    "    \n",
    "    \n",
    "\n",
    "    t2=time.time()\n",
    "    for i in range(len(all_lines)):\n",
    "        all_lines[i] = all_lines[i][0]+\"  \"+all_lines[i][1]        \n",
    "\n",
    "    #[s3] write to output file: output<id>  \n",
    "    s3_client = boto3.client('s3')\n",
    "    file_name = 'output/sorted_output'\n",
    "    for i in range(4):\n",
    "        with open(file_tmp, \"w\") as f:\n",
    "            start = 1000000*i\n",
    "            end = start + 1000000\n",
    "            f.writelines(lines[start:end])\n",
    "            result = s3_client.put_object(\n",
    "                Bucket = bucket_name,\n",
    "                Body = open(file_tmp, 'rb'),\n",
    "                Key = file_name + str(id*4+i)\n",
    "            )\n",
    "            os.remove(file_tmp)\n",
    "    t3=time.time()\n",
    "    \n",
    "    #return time (in sec) spent reading intermediate files\n",
    "    return [t1-t0, t2-t1_2, t3-t2, t1_2-t1] #read shuffle, compute, write output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_reducer(data):\n",
    "    t0=time.time()\n",
    "    n = data[0]\n",
    "    bucket_name = data[1]\n",
    "    \n",
    "    #read from input file: shuffle<0 id> shuffle<1 id> ... shuffle<id num_workers-1>\n",
    "    s3 = boto3.resource('s3')\n",
    "    tuples_list = []\n",
    "    file_name = 'output/sorted_output'\n",
    "    for i in range(n*4):\n",
    "        key = file_name + str(i)\n",
    "        s3.Bucket(bucket_name).download_file(key, key)\n",
    "\n",
    "    # concatenate all files \n",
    "    subprocess.call(\"cd output && cat sorted_output* > sorted_output\", shell=True) \n",
    "    \n",
    "    t1=time.time()\n",
    "    return (t1-t0)\n",
    "\n",
    "# final stage: concatenate outputs from the reduce/sort stage to form a single sorted output file\n",
    "#final_reducer([num_workers,bucket_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "wrenexec = pywren.default_executor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "map_data_list = []\n",
    "reduce_data_list = []\n",
    "sample_keys = []\n",
    "\n",
    "num_workers = 100\n",
    "bucket_name = 'terasort-yawen'\n",
    "path = '1TB/'\n",
    "\n",
    "for i in range(num_workers):\n",
    "    map_data_list.append([i, num_workers, bucket_name, sample_keys, path])\n",
    "    reduce_data_list.append([i, num_workers, bucket_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1_map = time.time()\n",
    "futures = wrenexec.map(mapper, map_data_list)\n",
    "results_map = pywren.get_all_results(futures)\n",
    "t2_map = time.time()\n",
    "print t2_map-t1_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1_reduce = time.time()\n",
    "futures = wrenexec.map(reducer, reduce_data_list)\n",
    "results_reduce = pywren.get_all_results(futures)\n",
    "t2_reduce = time.time()\n",
    "print t2_reduce-t1_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map:\n",
      "read input: 9.87234793854  max: 15.3644170761\n",
      "compute: 4.00935843515  max: 8.18745207787\n",
      "prepare: 1.81250811386  max: 3.52183794975\n",
      "write inter: 7.65488364458  max: 11.2715859413\n",
      "map_total: 23.3490981321  max: 34.2560219765\n"
     ]
    }
   ],
   "source": [
    "t_io = []\n",
    "t_comp = []\n",
    "t_inter = []\n",
    "t_prepare = []\n",
    "t_total = []\n",
    "for r in results_map:\n",
    "    t_io.append(r[0])\n",
    "    t_comp.append(r[1])\n",
    "    t_inter.append(r[2])\n",
    "    t_prepare.append(r[3])\n",
    "    t_total.append(r[0]+r[1]+r[2]+r[3])\n",
    "print \"map:\"\n",
    "print \"read input: \" + str(sum(t_io) / len(t_io)) + \"  max: \" + str(max(t_io))\n",
    "print \"compute: \" + str(sum(t_comp) / len(t_comp)) + \"  max: \" + str(max(t_comp))\n",
    "print \"prepare: \" + str(sum(t_prepare) / len(t_prepare))  + \"  max: \" + str(max(t_prepare))\n",
    "print \"write inter: \" + str(sum(t_inter) / len(t_inter))  + \"  max: \" + str(max(t_inter))\n",
    "print \"map_total: \" + str(sum(t_total) / len(t_total))  + \"  max: \" + str(max(t_total))\n",
    "\n",
    "# returns time spent (in sec) writing intermediate data in each mapper \n",
    "#results_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns time spent (in sec) reading intermediate data in each reducer \n",
    "\n",
    "t_io = []\n",
    "t_comp = []\n",
    "t_inter = []\n",
    "for r in results_reduce:\n",
    "    t_io.append(r[2])\n",
    "    t_comp.append(r[1])\n",
    "    t_inter.append(r[0])\n",
    "    t_total.append(r[0]+r[1]+r[2])\n",
    "print \"reduce:\"\n",
    "print \"read inter: \" + str(sum(t_inter) / len(t_inter)) + \"  max: \" + str(max(t_inter))\n",
    "print \"compute: \" + str(sum(t_comp) / len(t_comp)) + \"  max: \" + str(max(t_comp))\n",
    "print \"write output: \" + str(sum(t_io) / len(t_io)) + \"  max: \" + str(max(t_io))\n",
    "print \"reduce_total: \" + str(sum(t_total) / len(t_total))  + \"  max: \" + str(max(t_total))\n",
    "\n",
    "#results_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
