{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywren\n",
    "import numpy as np\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "import pywren.storage as storage\n",
    "import boto3\n",
    "import pickle \n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import redis\n",
    "from rediscluster import StrictRedisCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration \n",
    "num_workers = 3\n",
    "bucket_name = 'terasort-yawen'\n",
    "\n",
    "# the file to be sorted should be partitioned into \"num_worker\" number of files \n",
    "# as inputs to the map stage; \n",
    "# specify directory that contains files to be sorted: input1, input2, etc. \n",
    "path_local = 'input_files/' \n",
    "path = \"input_3_3M/\"\n",
    "file_name = 'input'\n",
    "concat_file_name = path_local + path + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# upload n input files to S3 (inputs to the mapper stage)\n",
    "for i in range(num_workers):\n",
    "    result = s3_client.put_object(\n",
    "        Bucket = bucket_name,\n",
    "        Body = open(concat_file_name + str(i), 'rb'),\n",
    "        Key = path + file_name + str(i)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## sample[i âˆ’ 1] <= key < sample[i] is sent to reduce i\n",
    "def get_sample_keys(file_path, num_workers):\n",
    "    f = open(file_path, \"r\")\n",
    "    lines = f.readlines()\n",
    "\n",
    "    key_list = []\n",
    "\n",
    "    for line in lines: \n",
    "        data = line.split(\"  \")\n",
    "        key = data[0]\n",
    "        key_list.append(key)\n",
    "\n",
    "    key_list.sort()\n",
    "    length = len(key_list)\n",
    "    print \"num records: \" + str(length)\n",
    "    n = num_workers\n",
    "    key_range = length/n\n",
    "    index = 0\n",
    "    sample_key_list = []\n",
    "    for i in range(1, n+1): \n",
    "        if (i==n):\n",
    "            index = length -1\n",
    "            sample_key_list.append(key_list[length-1])\n",
    "        else:\n",
    "            index += key_range\n",
    "            sample_key_list.append(key_list[index])\n",
    "        # print index\n",
    "    \n",
    "    return sample_key_list\n",
    "\n",
    "#get_sample_keys('input_files/input0', num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition stage: partition input data into n groups \n",
    "def mapper(data):\n",
    "    id = data[0]\n",
    "    n = num_workers = data[1]\n",
    "    bucket_name = data[2]\n",
    "    sample_keys = data[3]\n",
    "    path = data[4]\n",
    "\n",
    "    t0=time.time()\n",
    "    #[s3] read from input file: input<id> \n",
    "    s3 = boto3.resource('s3')\n",
    "    key = path + 'input' + str(id)\n",
    "    file_local = '/tmp/input_tmp'\n",
    "    s3.Bucket(bucket_name).download_file(key, file_local)\n",
    "    t1=time.time()    \n",
    "        \n",
    "    #partition \n",
    "    with open(file_local, \"r\") as f: \n",
    "        lines = f.readlines() #each line contains a 100B record\n",
    "    os.remove(file_local)\n",
    "    p_list = [[] for x in xrange(n)]  #list of n partitions\n",
    "    for line in lines:\n",
    "        key = line[:10]\n",
    "        index = 0\n",
    "        while key > sample_keys[index]:\n",
    "            index += 1\n",
    "        p_list[index].append(line)\n",
    "    t2=time.time()\n",
    "    '''\n",
    "    #write to output files in tmp: shuffle<id 0> shuffle<id 1> shuffle<id num_workers-1>\n",
    "    f_list = [] #output file list\n",
    "    #redis_client = redis.Redis(host=\"rediscluster.a9ith3.clustercfg.usw2.cache.amazonaws.com\", port=6379)\n",
    "    startup_nodes = [{\"host\": \"rediscluster.a9ith3.clustercfg.usw2.cache.amazonaws.com\", \"port\": \"6379\"}]\n",
    "    redis_client = StrictRedisCluster(startup_nodes=startup_nodes, decode_responses=True, skip_full_coverage_check=True)\n",
    "    \n",
    "    for i in range(n):\n",
    "        key = 'tmp/shuffle' + str(id) + str(i)\n",
    "        result = redis_client.set(key, pickle.dumps(p_list[i]))\n",
    "    '''\n",
    "\n",
    "    t3 = time.time()\n",
    "    \n",
    "    #return time spent (in sec) writing intermediate files \n",
    "    return [t1-t0, t2-t1, t3-t2] #read input, compute, write shuffle \n",
    "\n",
    "#mapper([0, num_workers, bucket_name, sample_keys, path])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort stage: merge n sets of data & sort \n",
    "def reducer(data):\n",
    "    id = data[0]\n",
    "    n = num_workers = data[1]\n",
    "    bucket_name = data[2]\n",
    "    \n",
    "    #read from input file in tmp: shuffle<0 id> shuffle<1 id> ... shuffle<id num_workers-1>\n",
    "    t0 = time.time()\n",
    "    #redis_client = redis.Redis(host=\"rediscluster.a9ith3.clustercfg.usw2.cache.amazonaws.com\", port=6379)\n",
    "    startup_nodes = [{\"host\": \"rediscluster.a9ith3.clustercfg.usw2.cache.amazonaws.com\", \"port\": \"6379\"}]\n",
    "    redis_client = StrictRedisCluster(startup_nodes=startup_nodes, decode_responses=True, skip_full_coverage_check=True)\n",
    "\n",
    "    lines_list = []\n",
    "    for i in range(n):\n",
    "        key = 'tmp/shuffle'+ str(i) + str(id)\n",
    "        body = redis_client.get(key)\n",
    "        if body == None:\n",
    "            return -1\n",
    "        lines = pickle.loads(body)\n",
    "        lines_list.append(lines)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    #merge & sort \n",
    "    merged_lines = sum(lines_list, [])\n",
    "    tuples_list = []\n",
    "    for line in merged_lines:\n",
    "        tuples_list.append((line[:10], line[12:]))\n",
    "    \n",
    "    sorted_tuples_list = sorted(tuples_list, key=lambda x: x[0])\n",
    "    t2=time.time()\n",
    "    \n",
    "    #[s3] write to output file: output<id>  \n",
    "    with open('/tmp/sorted_output', 'w+') as f:\n",
    "        for t in sorted_tuples_list: \n",
    "            f.write(t[0]+'  '+t[1])\n",
    "    \n",
    "    s3_client = boto3.client('s3')\n",
    "    result = s3_client.put_object(\n",
    "        Bucket = bucket_name,\n",
    "        Body = open('/tmp/sorted_output', 'rb'),\n",
    "        Key = 'output/sorted_output' + str(id)\n",
    "    )\n",
    "    t3=time.time()\n",
    "    \n",
    "    #return time (in sec) spent reading intermediate files\n",
    "    return [t1-t0, t2-t1, t3-t2] #read shuffle, compute, write output \n",
    "\n",
    "#reducer([0, num_workers, bucket_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def final_reducer(data):\n",
    "    import os\n",
    "    \n",
    "    t0=time.time()\n",
    "    n = data[0]\n",
    "    bucket_name = data[1]\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    for i in range(n):\n",
    "        key = 'output/sorted_output'+ str(i)\n",
    "        file_local = 'output/sorted_output'+ str(i)\n",
    "        s3.Bucket(bucket_name).download_file(key, file_local)        \n",
    "    \n",
    "    # concatenate all files \n",
    "    subprocess.call(\"cd output && cat sorted_output* > sorted_output\", shell=True) \n",
    "    \n",
    "    t1=time.time()\n",
    "    return (t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "wrenexec = pywren.default_executor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num records: 30000\n"
     ]
    }
   ],
   "source": [
    "map_data_list = []\n",
    "reduce_data_list = []\n",
    "\n",
    "sample_keys = get_sample_keys(concat_file_name, num_workers)\n",
    "\n",
    "for i in range(num_workers):\n",
    "    map_data_list.append([i, num_workers, bucket_name, sample_keys, path])\n",
    "    reduce_data_list.append([i, num_workers, bucket_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "futures = wrenexec.map(mapper, map_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_map = pywren.get_all_results(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "futures = wrenexec.map(reducer, reduce_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_reduce = pywren.get_all_results(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map:\n",
      "read input: 0.323478062948\n",
      "compute: 0.00417224566142\n",
      "write inter: 1.03314717611e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.23617792129516602, 0.003701925277709961, 1.1920928955078125e-06],\n",
       " [0.348499059677124, 0.004467964172363281, 9.5367431640625e-07],\n",
       " [0.3857572078704834, 0.0043468475341796875, 9.5367431640625e-07]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_io = []\n",
    "t_comp = []\n",
    "t_inter = []\n",
    "for r in results_map:\n",
    "    t_io.append(r[0])\n",
    "    t_comp.append(r[1])\n",
    "    t_inter.append(r[2])\n",
    "print \"map:\"\n",
    "print \"read input: \" + str(sum(t_io) / len(t_io))\n",
    "print \"compute: \" + str(sum(t_comp) / len(t_comp))\n",
    "print \"write inter: \" + str(sum(t_inter) / len(t_inter))\n",
    "\n",
    "# returns time spent (in sec) writing intermediate data in each mapper \n",
    "results_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce:\n",
      "read inter: 0.155095656713\n",
      "compute: 0.0137510299683\n",
      "write output: 0.361885945002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.17439794540405273, 0.014990091323852539, 0.5106229782104492],\n",
       " [0.07297301292419434, 0.009698152542114258, 0.21471381187438965],\n",
       " [0.21791601181030273, 0.01656484603881836, 0.360321044921875]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns time spent (in sec) reading intermediate data in each reducer \n",
    "\n",
    "t_io = []\n",
    "t_comp = []\n",
    "t_inter = []\n",
    "for r in results_reduce:\n",
    "    t_io.append(r[2])\n",
    "    t_comp.append(r[1])\n",
    "    t_inter.append(r[0])\n",
    "print \"reduce:\"\n",
    "print \"read inter: \" + str(sum(t_inter) / len(t_inter))\n",
    "print \"compute: \" + str(sum(t_comp) / len(t_comp))\n",
    "print \"write output: \" + str(sum(t_io) / len(t_io))\n",
    "results_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.941709041595459"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final stage: concatenate outputs from the reduce/sort stage to form a single sorted output file\n",
    "final_reducer([num_workers,bucket_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
